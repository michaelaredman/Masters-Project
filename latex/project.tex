\documentclass[11pt]{report}
\usepackage[a4paper]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathabx}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{relsize}
\usepackage{subfig}
\usepackage{graphicx}
\graphicspath{ {images/}}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{booktabs}


\usepackage[
authordate,
maxcitenames=1,
backend=biber,
natbib,
maxbibnames=6
]{biblatex-chicago}
\addbibresource{project.bib}
\bibliography{project}

\usepackage[usenames,dvipsnames]{color}
\usepackage{tikz}
\usepackage{listings}
\lstset{language=Python, breaklines=true}  
\usepackage{fancyhdr}
\setlength{\parindent}{0pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{Michael Redman, CID:\ 00826863}
\fancyfoot[C]{\thepage}
\begin{document}
\title{Detecting pathologies in spatiotemporal data}
\author{Michael Redman, CID:\ 0082686\textbf{3}}
\date{December 2016}

\maketitle

\tableofcontents

\chapter{Introduction}

The identification of unusual patterns of disease is of obvious importance in public health. Discrepencies in the relative risk of disease incidence between regions will both inform the allocation of medical resources and motivate investigations over possible environmental exposure. \\

 The detection of such discrepencies is often done by the use of disease mapping studies which aggregate the recorded incidents of the disease of interest over time, and compare the total counts between regions. Public health information systems now record this data indexed to such a fine degree of granularity that the detection of risk factors which act at a very local level, such as hazardous industrial waste, can be feasably indentified. However this fine-grained approach will leave the examined regions with very low incidence, which can hinder the isolation of the signal from the noise, hence the use of time aggregated values. However, in doing this we relinquish any potential information that a temporal trend may afford us. In fact, as described in \citet{stability}, the nature of the temporal trend may suggest anomalities related to very different kinds risk factors. A region which conforms to a similar temporal trend as the others---for example a seasonal factor---but at a higher rate would imply a disparity which acts over the whole time period, such as an enviromental issue or a sociodemographic difference. While a region which acts wildly without regard to the general trend might suggest a more acute issue is at play. \\

Consequently, in this project we will be looking at data that is indexed in both space and time. Additionally in any model that we implement, we wish to be able to control for known covariates which may influence the risk within a given region. For example some regions will be more populous, while others may contain proportionally more at-risk peoples, so it would be wise to have some method of setting the expected counts by region. \\

A variety of methods have been investigated in these contexts. One of the most simple is that of scan statistics. This procedure, implemented in software such as SaTScan, seeks to identify clusters which can not be explained by the base process -- in our case typically an inhomogenous Poisson process. It does this by evaluating all possible ``windows'' in turn, where a given window will include the region in question and its ``neighbours'' both spatially and temporally. This forms a kind of cylinder which acts to smooth the observed counts in space and time, allowing the sparse data as described earlier to be examined without the noise overwhelming any underlying spatial heterogenicity. This method is descibed in detail and applied to epidemiological data in \citet{scan}. While offering an improvement over pointwise evaluation, spatial scan statistics by nature of this smoothing construction are limited in their ability to detect abnormalities at the individual level, i.e. isolated to a particular region \citep{baystdetect}.  \\

Control chart based methods offer both relatively fast computation times and the ability to specifically identify abnormal temporal trends. These, in the simplest of terms, are graphs we construct combined with a decision rule to classify whether a process is in `in-control' or `out-of-control'. Versions such as CUSUM are often designed for step detection -- the identification of abrupt jumps in rate of the process. When looking at longer time periods, this is exactly the type of pathology one might expect. The regions which now show atypical trends should still have exhibited normal behaviour most of the time, the problem is to decide when such heterogeneity in the process is substantive.  Another advantage is, as the techinique is sequential in nature, the sample size of the items in consideration need not be fixed in advance.  \\

The most natural way to frame our problem is by the construction of a hierachical model. In this method we perform inference on a latent variable---within a complex model---which indicates the `unusalness' of a point or group of points. This Bayesian approach compells us to express our assumptions about the data generatively, which confers a number of advantages. Firstly, it is often the case in discriminative models that the choice of method is implicitly performing shrinkage, without this being made explicit or even realized by the modeler. For an intereting example see \citet{stoch}. Thinking generatively forces us to be clear about our assumptions. Secondly, real world epidemiological data will presumably not supply the true response variable, that is, whether or not a region is genuinely anomalous. This abscence of training means we need our model to provide reasonable estimates immediately, rather than at some asymptotic limit. This advantage can be seen in, for example, \citet{ng}. Thirdly, the parameters of a Bayesian model are far more readily interpretable that those in a non-generative setting, which could allow for further inference beyond the identification of abnormal regions, e.g. the nature and extent of the deviation from what was expected. And lastly,
 the Bayesian framework naturally allows the addition of futher assumptions and complexity. For example, \citet{banerjee} extends spatiotemporal models to the modelling of multiple diseases via the introduction of a shared `frailty' term. \\

The examination, implementation and comparison of these Bayesian methods with discriminative models, specifically control charts, is the focus of this project. \\

Working on simulated data, we first implement a CUSUM based classifier and suggest a novel(?) specification of the `in-control' process. With the efficiency of the method being of great importance, we implement the computations from scratch in Fortran rather than modifying the existing statistical routines in R. \\

Then, turning to Bayesian methods, we examine the general structure of the models employed in the literature and how this standard is well suited to the particular challenges of our problem. In particular, the favored priors for the smoothing of the data are reviewed. The difficulties and subtleties of computaion are addressed in detail, as are some of the superior techniques that have been developed in the past few years. \\

We examine BaySTDetect, one of the existing models in the literature, discussing the rationales behind the construction of the model and the `cutting of feedback' that's employed -- where we demonstrate that this particular choice is fundamental to it's success. Rather than use the standard MCMC software in the field, we implement the model in Stan using Hamiltonial Monte Carlo and combine the submodels directly through the likelihoods. We show that this can lead to order of magnitude speed-ups over the original implementation in \citet{baystdetect}. \\

We then suggest a new model, based on \citet{stability}, that works fully within the Bayesian framework by identifying an excess in the variance which can not be explained by the shared random effects. Implementing this model, again in Stan, we show that this model obtains good results, despite not being the way in which our simulated data were originally created. However, as suggested in \citet{best2005comparison}, models of this kind may oversmooth the counts spatially. We demonstrate this by simulating a new set of data where the regions to be `unusual' are selected sequentially via a preferential attachment scheme and showing that our model performs poorly in this context. \\

Finally we compare the use three models on both sets of data with respect to their speed, false postive rate and power.

\chapter{Simulated data}

In order to test the accuracy of any potential models we need some data that exhibits the type of pathological trends that we wish to identify. The following data was graciously supplied by Areti Boulieri.

\section{Specification}

A real dataset of asthma hospitalisation counts was used to contruct the expected rates, adjusted by regional demographics by age and sex, for the 221 Clinical Commisioning Groups in England. These are the expected counts that we will standardize against when attempting to identify abnormalities. We denote these values throughout by $E_{i}, \text{ for } i=1,\ldots,221$. A plot that we generated to show the distribution of these values across the regions can be seen in fig \ref{}  \\

A binomial random variable might be the most principled choice of response but the expected counts we have calculated are large enough for using a Poisson response to be essentially equivalent. Indeed this is the most popular choice in the literature. The data was generated across 15 time points, so our values are drawn from
\begin{equation}
Y_{i,t} \; \sim \; \operatorname{Poisson}(E_i \cdot \mu_{i,t}), \ \ \ \ i=1,\ldots,221 \ \ \ \ t=1,\ldots,15
\end{equation} 

where $\mu_{i,t}$ allows us to add random effects to our model. \\

These random effects are used to add additional natural variation to the counts, where the direction of this variation is in some sense shared between `similar' regions or time points. We seperate these effects into those that describe the temporal trend and those which affect the spatial heterogeneity. So for the temporal trend we of course expect adjacent time points to be similar. This is achieved in this data by sampling from a one-dimensional Gaussian random walk
\begin{equation}
\xi_{1:15} \; \sim \; \operatorname{RW}_1(\sigma_{\xi}^2).
\end{equation}

Additionally, we would expect regions which are close to each other to be similar. The data generation incorporated this assumption by the use of the BYM prior, first defined in \citet{bym},
\begin{equation}
\lambda_{1:211} \; \sim \; \operatorname{BYM}(W, \sigma_{\lambda}^2, \sigma_v^2) 
\end{equation}
which is a multivariate normal distribution
\begin{equation}
\lambda_{1:211} \; \sim \; \mathcal{N}(v_{1:211}, I_{211} \cdot\sigma_{\lambda}^2)
\end{equation} 
where the mean vector is drawn from an \emph{intrinsically autoregressive} process
\begin{equation}
v_{1:211} \; \sim \; \operatorname{IAR}(W, \sigma_v^2)
\end{equation}
which is a Markov random field where areas that are adjacent (according to an adjacency matrix $W$) are more similar to those further away. The precise specification of the BYM prior is discussed in section \ref{carmodel}. \\

The presumed sources of these random effects will determine how we wish for them to influence the generation of the data. In this dataset it is assumed that the temporal and spatial effects will work multiplicatively, where increases in one will magnify the effect of the other. Therefore, given our samples from the aformentioned distributions, we calculate $\mu_{i,t}$ additively on the $\log$ scale
\begin{equation}
\log{(\mu_{i,t})} = \lambda_i + \xi_t \ \ \ \ i=1,\ldots,221 \ \ \ \ t=1,\ldots,15
\end{equation}

In total, this scheme describes a method of generating random variables that has the features one would expect of spatiotemporal disease incidence. We now need to modify a selection of the regions to resemble a regions we would expect to be unusual. \\

Fifteen of the regions were chosen according to the 10th, 25th, 50th, 75th and 90th percentiles of the median expected counts over time, with three regions per percentile. At each percentile the three regions were selected corresponding to one of three levels of spatial risk, i.e. the generated spatial effect $\lambda_{1:211}$, low (10th-30th percentiles), medium (45th-55th percentile) and high (70th-90th percentiles). This was done to ensure that the unusual regions were spread evenly given the underlying levels of risk. \\

To make these regions unusual they were given a deviant temporal trend. This was done by modifying the temporal trends of these regions manually, where---writing the original trend as $g(t)$ and the modified trend as $g^*(t)$---we have:
\begin{align}
g^*(t) &= g(t) + \log(2) && t = 1, 10, 11 \\ 
g^*(t) &= g(t) - \log(2) && t = 5, 15
\end{align}

This has the effect of either doubling or halving the expected counts depending on the time point. The resulting temporal trend compared to the original can be seen in figure \ref{fig:timeeffects}. \\

 N.B. All of the unusual regions exhibit the same deviation, which is probably an unrealistic assumption. However, by the way that they're defined, it is clear that the models used in this project should not perform better because of this fact. 

\begin{figure}
\includegraphics[scale=0.5]{plot_time_effects}
\label{fig:timeeffects}
\centering
\end{figure}

\begin{itemize}
\item Check this trend with Marta as the word document and pictured trend are different.
\end{itemize}

\section{Cleaning and wrangling}

Of the 211 regions only one one had no neighbours, the Isle of Wight. As this will complicate some of the smoothing calculations we need to perform for the Bayesian procedures (and the Isle of Wight was not one of those selected as unusual), the region was removed to simplify calculations.

Shape data for the 211 Clinical Commisioning Groups plus wales was provided by CITE HERE. The regions of Wales and the Isle of Wight were of course removed. The shapefiles were imported and edited using the package \emph{rgdal} created by \citet{shaperead}. Then, using the package \emph{spdep} created by \citet{shape}, the shapefile was used to generate an object which calculates all of the adjacent regions for each region. Then this object was converted into into an adjacency matrix $W$ such that
\begin{equation*}
{(W)}_{ij} &= 
\begin{cases}
1, \ \ i \leftrightarrow j \\
0, \ \ \textrm{otherwise}
\end{cases}
\end{equation*}
where the double arrow signifies adjacency. The result of this procedure can be seen over the original shapedata in fig \ref{adj}.


\begin{figure}
\centering
\subfloat[Adjacency]{\includegraphics[width=0.5\textwidth]{adjacency}}
\subfloat[Expected]{\includegraphics[width=0.5\textwidth]{expected}}
\caption{Mapped data}
\end{figure}

\chapter{Control charts}


The use of control charts are widespead in the area of statistical quality control and have recently increasingly found use in fields such as epidemiology, see for example \citet{mei}. Their relative simplicity and ease of computation compared to most other methods makes them an ideal model to compare against, in order to observe any gains we might obtain from more complex modelling. Indeed, as expressed by \citet{vapnik} in the context of generative vs discriminative models, ``one should solve the problem directly and never solve a more general problem as an intermediate step''. \\

\section{CUSUM}

We will specifically be looking at the use of CUSUM models, which seek to identify a qualitative change in the nature of a time series via the evaluation of the cumulative sum of a relavant test statistic. In the setting of the inhomogenous Poisson CUSUM, we assume that the data follows a null model defined by a series of rate parameters which define the `in-control' Poisson process. Likewise we have a complementary set of parameters that define the rates of an `out-of-control' process. We wish to detect the point in time at which there is sufficient evidence that the process was switched from the null to the `out-of-control' process, as quickly as possible while not exceeding some measure of how often we err. \\

To do this, define $S_t$ as the value of the control chart at time $t$ and progress its value as follows
\begin{gather}
S_0 = 0 \\
S_{t+1} = \max{(0, S_t + K_{t+1})}.
\end{gather}
where $K_t$ is the value of a statistic calculated at time point $t$. \\

As first proposed in \citet{page}, we will use
\begin{align}
K_t &= \log{\left(\frac{f_1(Y_{i,t})}{f_0(Y_{i,t})}\right)} \\
    &= \log{[f_1(Y_{i,t})]} - \log{[f_0(Y_{i,t})]}.
\end{align}
where $f_0$ is the likelihood of the 'in-control' rate and $f_1$ that of the 'out-of-control'. \\

That is, the $\log$ of the ratio between the likelihoods of the data, at the time point, under the `in-control' and `out-of-control' Poisson process. So if likelihood of the `out-of-control' rate is greater than the `in-control' then the value of $S_t$ will increase and vice-versa. We then signal that the process is pathological if the value of $S_t$ exceeds some threshold value $h$. \\

Note that, for a Poisson process, the likelihood is
\begin{equation}
f_j(x) = \frac{\lambda_j^x e^{-\lambda}}{x!}.
\end{equation}
So for the log-likelihood we have
\begin{equation}
\ell_j(x) = x \log(\lambda_j) - \lambda_j - \log(x!)
\end{equation}
giving the value of our test statistic as
\begin{equation}
K_t = Y_{i,t} (\log{\lambda_{1, t}} - \log{\lambda_{0, t}}) - (\lambda_{1, t} - \lambda_{0, t}).
\end{equation}

This leaves us to come up with sensible methods of generating the `in-control' and `out-of-control' rates and the threshold value that we trigger at. With the data that we are examining in this project we have the expected values of the disease rates in each region and could use these for the values of the `in-control' rate but this would neglect the fact that we expect the incident counts at each region to vary over time and---importantly---this is not neccasarily pathological behaviour. It is instead a temporal trend which differs from some general (here country-wide) trend which we wish to detect. Therefore the method we propose here is to construct a general temporal trend from an average across the regions and to weight this trend per region by that regions expected counts. \\

So for the data $Y_{i,t}, \ i = 1, \ldots, R, \ t = 1, \ldots, T$ we normalize the temporal pattern of each regions by its expect count $E_{i}$ as follows
\begin{equation}
\tilde{Y}_{i, t} = \frac{Y_{i, t}}{E_{i}}, \ \ \ \ i = 1, \ldots, R, \ t = 1, \ldots, T.
\end{equation} 

We then construct the `in-control' rate for each region $I_{i, t}$ as described
\begin{equation}
I_{i, t} = E_i \cdot \frac{1}{R} \sum_{i=1}^N \tilde{Y}_{i, t}, \ \ \ \ i = 1, \ldots, R, \ t = 1, \ldots, T.
\end{equation}

For simplicity we only consider the case of abnormally high count rates and ignore artificially low counts although this could be incorporated into the CUSUM model if desired. So to define the `out-of-control' rate it makes sense to follow the same temporal trend but with a modified general rate. We do this by calculating the `out-of-control' rate $O_{i,t}$ as a multiple of the `in-control' as follows
\begin{equation}
O_{i,t} = \alpha \cdot I_{i,t}, \ \ \ \  i = 1, \ldots, R, \ t = 1, \ldots, T.
\end{equation} 
with $\alpha > 1$. \\

Now for a given value of $\alpha$ we need a threshold level at which to signal. This can be done by caluclating the average-run length (ARL) which is the expected length of the series between flags of the model. The ARL on the `in-control' rate will then give a measure of the false positives for a given threshold $h$, which we can optimize with respect to. As the length of the series we are examining is fixed and the calculation of an ARL would not be clearly defined for a series of varying expectation we will not use the ARL metric here. Instead we simply simulate the `in-control' sequence for each region a large number of times and find the smallest value of $h$ for which the the false-postive rate is below some desired rate. \\

The setup as described was implemented as a Python class acting as a wrapper to a series of subroutines in Fortran via F2PY\footnote{I originally wrote the entire process in Python but the calculation of the $h$ values was far too slow to be practical. The original code can be on my Github.}. The code can be found in the appendix. \\

This CUSUM process was applied to the asthma data with the ratio $\alpha$ set to $1.5$, indicating we consider the ``out-of-control'' rate to be 50\% greater than the ``in-control'', with each regions temporal pattern being simulated 10,000 times to determine the optimal threshold values. This identified 25 regions as ``out-of-control'' out of the total 210, giving a false-negative rate of 0\% and false-positive rate of 5.13\%. \\

The power of the test is clearly excellent, but the false-positive rate is clearly much greater than our desired limit of 1\%. Why is this? \\  

Note that the expected value of $K_t$ under the `out-of-control' model ($H_1$) will be the Kullback-Leibler divergence between the two models \citep{weighed},
\begin{align*}
\mathbb{E}[K_t | H_1] &= \int_{-\infty}^{\infty} f_1(x_t) K_t \; dx_t \\
                        &= \int_{-\infty}^{\infty} f_1(x_t) \log{\left(\frac{f_1(x_t)}{f_0(x_t)}\right)} \; dx_t \\
                        &= D_{\mathrm{KL}}(H_1\|H_0).
\end{align*}

This gives an intuitive explanation of the procedure: under the alternate (`out-of-control') model, $K_t$ measures the information lost by attempting to approximate the (true) alternate model by the null. When the cumulative information lost by this approximation becomes intolerable, we reject the null. \\

It can be shown that, under some assumptions, using this metric with the CUSUM method is in some sense optimal \citep{ritov1990}. However, the issues with the assumptions here are clear. The null model as we have defined attempts to incorporate the general temporal trend but we would also expect a level of random spaital variation which masks the underlying abnormalities and our procedure ignores this. Therefore for regions which are not unusual (call these $H_G$), the null model will underestimate the likelihood ($f_0(x_t)$) of their counts relative to the true likelihood $f_G(x_t)$. So, under $H_G$ one would expect $\log(f_1(x_t)) - \log(f_0(x_t))$ to be large more often than under the null, underestimating the value of $h$ which would be required to keep the FPR under 1\%. \\

Additionally, even if the true data is distributed as we assume in the null, the parameters of the temporal trend that defines the `in-control' models are still an estimation from the data. The errors between this hypothetical `true' temporal trend and our estimate from the data will add additional noise into the process. In particular, this will affect the estimation of the optimal value of $h$ for a given FPR -- potentially increasing the number of false positives relative to what we might otherwise expect. \\

A number of methods exist that attempt to account for this additional estimation error. For example, following the method devised in \citet{gandy}, we could estimate the `in-control' parameters as usual and then draw samples with replacement from the `in-control' model with these parameters. Then, calculating estimates of the `in-control' parameters from these samples, we obtain a series of bootstrap estimators. The optimal values of $h$ for each of these bootstrap estimators can be used to devise a threshold which posseses our desired properties with a certain probability. Of course, the data from which we estimate the temporal trend is not all `in-control', so our estimates will be even more biased than this would suggest. 
 
\section{Spatial CUSUM}

While our method accounts for a varying temporal trend, it falls short most saliently by not accounting for the spatial effects we assume our data posseses. The approach suggested by \citet{raubertas} attempts to take the spatial information into account by replacing the counts at each region by a weighted sum
\begin{equation*}
x_{i,t}' = \sum_{j=1}^R s_{i, j} x_{j, t}
\end{equation*} 
where we pool within neighbourhoods by assigning the weights based on the `closeness' of the regions. \\

For example this method was used in \citet{spatialcusum} with the Poisson CUSUM, where the counts at each region were replaced by
\begin{equation*}
x_{i,t}' = \sum_{x_{j, t} \in \partial x_{i,t}} x_{j, t} 
\end{equation*} 
where $\partial x_{i,t}$ are all the regions including and adjacent to $x_{i,t}$. \\

The Poisson CUSUM statistics were calculated for these modified regions and the Benjamin-Hochberg procedure was used to determine the level at which we signal for a given false discovery rate. Despite this methods simplicity, the author uses a set of simulated data to show that it can provide a significant improvement on considering the regions individually. \\ 

This approach could be used in conjunction with that of the previous section to account for both spatial and temporal effects, while still enjoying the benefits of CUSUM. Note that, in this case, the expected counts would also need to be summed across neighbourhoods to provide the `in-control' means. 

\chapter{The Bayesian Framework}

A sensible way of improving on this would be to use a generative model which incorporates both a flexibility that describes our uncertainty around the parameterization of the model and makes explicit our beliefs as to the structure of the data. This is most naturally done in the Bayesian framework, where we specify the model in terms of a likelihood which describes how we simulate the data based on a set of parameters and a collection of---possibly hierarchical---priors which describe our uncertainty about these parameters ex ante. 

\begin{enumerate}
\item Advantages of generative models
\item Advantages over frequentist approaches, e.g. mean/mode often terrible approximations, in part due to concentration of measure
\item Level of shrinkage attributable to closeness of prior structure to some ``true prior''
\item Bayes formula
\item Difficulty in evaluating integrals/expectations
\item Samples will converge to expectation (use betancourts conceptual into here)
\item Proposal density and preserving the pdf
\item Metropolis-hastings
\item Theory and issues including: large/small step sizes, large autocorrelation, slow exploration of the sample space, divergences
\item Gibbs sampleing and blocked gibbs: overcomes some element of dimensionality but poor for highly correlated variables, compounded by issues with 
\item Asymptotics not followed in finite time often
\item Quick talk about existing software
\item Hamiltonian monte carlo provides some solutions to many of these problems
\item In last few years (get exact) emergence of better algorithms, tuning and software have made using HMC practical. Not used in epidemiology much yet though. Different fields different amounts due to issues with discrete paramters etc. that we will address later.
\item Convergence statistics
\end{enumerate}

\begin{itemize}

\item Space-time seperability

\item Identification issuses in mixture model

\item Bayesian model selection

\item Prior on mixture component probability

\item Bayesian classification

\end{itemize}

\subsection{Hyperparameter priors}

\begin{itemize}

\item Gelman 2006 for variance parameters

\item Stan wiki for others

\end{itemize}

\subsection{Convergence statistics}

\begin{itemize}

\item Trace plot

\item Gelman-Rubin statistic

\item Multiple-chains are run not for computational benefits but to assess convergence

\item Gelman-Rubin-brooks plot

\item Lack of divergences---which are ``incredibly sensitive to the kind of pathologies that can obstruct geometric ergodicity'' 


\end{itemize}


\subsection{Hamiltonian Monte-Carlo}

The previous methods, while possessing the desired asymptotic properties, can---and do---take a long time to converge to the target distribution (infinity is a long time!). The aim of \emph{Hamiltonian Monte-Carlo} is to reduce the correlations between successive samples and so dramatically increase the effective sample size with minimal computational overhead. It does this by introducing an auxilliary variable and exploiting the interplay between this and the variables of the posterioir distribution within the framework of Hamiltonian mechanics.

\subsubsection{Background}

We write $q$ for the variables of the posterior distribution and introduce a new variable $p$ which we will call the \emph{momentum} of the system. Drawing from statistical mechanics, we know that for a given energy function of a system $E(\theta)$ we have the canonical ensemble
\begin{equation}
  p(\theta) = \frac{1}{Z} e^{-E(\theta)}.
\end{equation}

So defining a Hamiltonian of our system where the ``kiniteic energy'' is dependent on our momentum and the potential energy is dependent only on the posterioir:
\begin{equation}
  H(p, q) = \underbrace{T(p|q)}_{\text{Kinetic energy}} + \underbrace{V(q)}_{\text{Potential energy}}
\end{equation} 

looking at the canonical ensemble of the join distribution
\begin{align}
  \pi(p, q) &\;{\propto} \; e^{-H(p, q)} \\
          &= e^{-[T(p|q) + V(q)]} \\
          &= e^{-T(p|q)} \cdot e^{-V(q)} \\
          &\;{\propto} \; \pi(p|q) \cdot \pi(q) 
\end{align}

we see that the posterior and distribution of the momentum are seperable and so independent. This means that if we can sample from $\pi(p, q)$ then our choice of distirbution for the momentum will not effect the calculation of any expectations based on the samples of $q$. \\

Note that here we need a definition of potential energy which will give us back the posterior. Clearly the following satisfies this requirement
\begin{equation}
  V(q) = - \log \pi(q)
\end{equation}

So draws from $\pi(p, q)$ will allow us to make inferences on the $q$, but what advantages does the introduction of this auxilliary variable confer? Well, continuing with our intuition regarding a physical Hamiltonian system, we know that $p$ and $q$ are related by Hamilton's equations
\begin{align}
\frac{dq}{dt} &= \frac{\partial H}{\partial p} = \frac{\partial T}{\partial p} \\
\frac{dp}{dt} &= - \frac{\partial H}{\partial q} = - \frac{\partial T}{\partial q} - \frac{\partial V}{\partial q}
\end{align}

\begin{itemize}
  \item Finish off this bit more gracefully
\end{itemize}

\subsubsection{The Method}
Hamiltonian monte-carlo works, not by applying a transition over $q_i$, but by giving our starting point ``a kick'' in the form of a random momentum and sampling along the level set of constant energy (defined by the Hamiltonian) using Hamilton's equations to evolve the joint system. Then---after some number of steps---we stop, sample a new momentum, and explore the new phase space that this defines. \\

The questions now are obvious:

\begin{itemize}
\item \emph{How} do we evolve the joint system in accordance with Hamilton's equations? (Leapfrog)
\item Do we need to work out all of the partial deriavtives $\partial V/\partial q_i$ by hand? 
\item For how long should we explore each level set before sampling a new momentum?
\item What proposal density should we use for the momentum?
\item Why does this method lead to samples with less autocorrelation?
\end{itemize}

Other things to talk about:
\begin{itemize}
\item \emph{How} do we evolve the joint system in accordance with Hamilton's equations?
\item Do we need to work out all of the partial deriavtives $\partial V/\partial q_i$ by hand? 
\item For how long should we explore each level set before sampling a new momentum?
\item What proposal density should we use for the momentum?
\item Why does this method lead to samples with less autocorrelation?
\item Parameter tuning vs burn-in period
\end{itemize}

Other things to talk about:
\begin{itemize}

\item Rotational invariance

\item Include some diagrams from Michael Betancourt's papers

\end{itemize}

\subsection{General Model Form}

\begin{itemize}
\item Use best et al to summarise basic model form
\end{itemize}

\chapter{Smoothing}

Ideally we wish to identify potential local risk factors in the aetiology of a disease, say, carcenogenic hazard from industrial polution. So it's clear that the ability to incorporate a high level of spatial granularity in our model is of value in these contexts. However this comes with the trade-off of greater variance in the counts, making identification of abnormal temporal trends difficult, especially for diseases with low incidence. Therefore we need to employ an element of smoothing over the local neighbourhoods of each region. This can be done in a variety of methods. One possibility is the use of splines such as in \citet{spline} but in this project we will primarily looking at the use of autoregressive priors. For a comparison of these two approaches see \cite{splinecompare}.   

\section{CAR models} \label{carmodel}

One of the most natural ways of describing the proximity of regional data is by their adjacency relationships. These can be described mathematically as an undirected graph where the edges connect those regions which share a border. When the probabilistic dependence of the regions on this graph extent only to those that share an edge (i.e. the Markov property), then we call the network a Markov random field. \\ 

In the Bayesian setting, the most popular way of modelling this interdependence is through the use of a conditionally autoregressive (CAR) prior. These CAR models can be best understood when specified in terms in terms of their conditional distribution
\begin{equation}
v_i \ | \ v_j \ j \neq i \sim N(\alpha \cdot \bar{\mu_i}, \ \sigma_v^2/k_i)
\end{equation}

where $k_i$ is the number of neighbours adjacent to region $i$,

\begin{equation}
\bar{\mu}_i = \sum_{j \in \partial i} \frac{\mu_j}{k_i}
\end{equation}

and $\alpha \in (0, 1)$ is a parameter measuring the degree of spatial dependence. \\

However, while this defines a Markov random field, it is not a directed acyclic graph and we can't use this definition in non-gibbs sampling methods -- we need the $v_i$ to be jointly specified. Thankfully it is possible \citep{banerjeebook} for this to be expressed in terms of a multivariate normal distribution as follows,
\begin{equation} \label{eq:car}
v \ \sim \ N(0, \ \sigma_v^2 \cdot {[D(I_n - \alpha B)]}^{-1})
\end{equation}

where

\begin{align}
D &= \operatorname{diag}(k_i) \\
B &= D^{-1} W \\
{(W)}_{ij} &= 
\begin{cases}
1, \ \ i \leftrightarrow j \\
0, \ \ \textrm{otherwise}
\end{cases}
\end{align} 

Note that it is intuitively clear that the precision matrix here in high dimensions will be sparse and so naive calculations will be very inefficient -- see the section on computational considerations for some more sophisticated methods that we will use to simulate the distribution. Cite \citet{car}

\subsection{Intrinsically autoregressive priors}

When we take the CAR distribution under the limiting case of $\alpha \to 1$ we obtain the intrinsically autoregressive prior (ICAR/IAR). The ICAR prior is very popular throughout disease mapping due to its use in the BYM prior described in the next section. The use of  


\begin{itemize}

\item Cite http://www.biostat.umn.edu/~brad/software/jbc.proofs.pdf


\item Spatial dependence parameter $\alpha$ --- to prior or not to prior?

\item Intrinsically autoregressive model --- improper as covariance matrix is semi-definite

\end{itemize}

\subsection{BYM prior}

\begin{itemize}
\item CIte Besag York Mollie paper
\item Increases degrees of freedom for spatial componenet
\end{itemize}

\subsection{Temporal smoothing}

Similarly to in the spatial setting we can use a prior on the temporal component that assumes a level of similarity between adjacent regions -- here consecutive time points. The prior prefered here is the one dimensional random walk prior, which we will denote by
\begin{equation}
\xi_{1:T} \sim \operatorname{RW}(1)
\end{equation}

where the dimensionality will often be infered from the context. Note that this can be represented by a CAR model and is sometimes represented as such in the literature. 

\chapter{Individual trend model}

The first Bayesian model we wil consider is similar to that which is formed in Someone et al. In this setting we construct two alternate hypothesis for each region, one where the counts at the region are broadly in keeping with some ``global'' temporal trend (subject to localised spatial deviations captured with a conditionally autoregressive prior), and in the other the region has its own individual temporal trend. Then by some method of classification we sort the regions into those deemed most likely to follow the global model and those exhibiting behaviour more typical of the second model - and label these regions ``unusual''.

\section{Baystdetect and the cut function}

In the original paper \citet{baystdetect} the use of the cut function in the \emph{BUGS} language is employed to fit the two models to the data seperately and then the model selection is undertaken afterward. This method, which prevents the flow of information between the two models is defended in (Nicky Best presentation here) but has been met with some level of skepticism in the community, for example in Andrew Gelmans posts to the Stan mailing list here (insert link), as the analysis is not ``truly Bayesian''. Nethertheless, we examine this paradigm and compare it to fully Bayesian methods. 

\begin{itemize}
\item Include discussion of model averaging vs larger model
\item Cite Gelman et al 2013
\end{itemize}

\section{Model specification}

We denote the counts at region $i$ at time $t$ by $Y_{i,t}$ and model them by a Poisson process

\begin{equation}
Y_{i,t} \sim \operatorname{Poisson}(E_{i,t} \cdot \mu_{i,t})
\end{equation}

where $E_{it}$ is the expected count based on population numbers, demographics etc and $\mu_{i,t}$ is the rate parameter by which we impute the two models behaviours. This rate variable we parameterize additively on the log scale for both models as follows

\begin{equation}
\log{(\mu_{i,t})} = \begin{cases}
\ \lambda_{i} + \gamma_{t} \  (+ \  \alpha_0) & \textrm{Model 1 for all } i, t \\
\ u_{i} + \xi_{i,t} & \textrm{Model 2 for all } i, t
\end{cases}
\end{equation} 

Here we see that for Model 1 we assume space-time seperability in the rate paramater with the components given the following priors

\begin{align}
\alpha_0 &\sim \operatorname{Flat}(\mathbb{R}) \\
v_{1:N} &\sim \operatorname{CAR}(W, \sigma_v) \\
\lambda_{1:N} &\sim \operatorname{Normal}(v, \sigma_\lambda) \\
\gamma_{1:T} &\sim \operatorname{RW}(1)
\end{align}

We see here the BYM prior on the spatial component, imposing a smoothing constraint, and a one-dimensional random walk prior on the temporal component. 

The variance hyperparameters are given (insert prior here) as recomended in (possibly Gelman 2006). \\

For the second model we drop the assumption of space-time seperability and each region gets its own temporal trend as follows

\begin{equation}
i \in 1,\ldots,N \
\begin{cases}
\ u_i \sim \operatorname{Normal}(0, 1000) \\
\ \xi_{i, 1:T} \sim \operatorname{RW}(1) 
\end{cases}
\end{equation}

\section{Combining the models}

\begin{itemize}
\item BUGS can be used with a mixture component
\item In this case the likelihood can be worked out by hand with the probability of the mixture component prior
\item Describe likelihood code
\end{itemize}

\subsection{Implementation}

The two models were fit to the data using Hamiltonian Monte-Carlo in the \emph{Stan} package -- both for over 2000 samples over 4 chains after a ``warm-up'' period of 1000 samples. Gelman-Rubin statistics for all parameters were under 1.05 (find exact figure) indicating that the had chains converged. Additionally a visual inspection of the trace plots for some select parameters did not indicate any worrying pathalogical behaviour.

These two fits took under 5 minutes each to run after a short compilation which is a huge speed increase over typical \emph{BUGS} implementation.

\section{Classification and accuracy}

First looking at the general trend model we see that it has accurately identified the global temporal trend. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{global_temporal}
\end{figure}

\section{Fully Bayesian model}

\begin{itemize}
\item Can be fit fully in Stan
\item Reference the use of Rao-Blackwellization
\item Graph the temporal components failure
\end{itemize}

\chapter{Excess variability model}

The second Bayesian model we will look at also attempts to classify the regions, into typical and atypical sets, using a mixture model. Here our abnormal regions, rather than coming from an individual temporal trend, simply have excess inseperable spatio-temporal variance. Some potential advantages of such a model compared to Baystdetect are immediately clear:
\begin{itemize}
\item It's relative simplicity allows for easier computation
\item It is not immediately clear that an abnormal temporal trend is symptomatic of an endemic problem --- variance is a more straightforwardly interpretable parameter
\item Identification issues
\item Mixture model issues reduced due to setup of variance
\end{itemize}

\section{Model specification}

The model is similar to that of \citet{stability}, but with a mixture component that's hierachical at the regional level. Like before we model the counts as a Poisson process, $Y_{i,t} \sim \operatorname{Poisson}(E_{i,t} \cdot \mu_{i,t})$, with a rate parameter defined additively on the log-scale

\begin{equation}
\log(\mu_{i,t}) = \lambda_i + \gamma_t + \psi_{i,t}
\end{equation}

where, as before,
\begin{align}
v_{1:N} &\sim \operatorname{CAR}(W, \sigma_v^2) \\
\lambda_{1:N} &\sim \operatorname{Normal}(v, \sigma_\lambda^2) \\
\gamma_{1:T} &\sim \operatorname{RW}(1).
\end{align}

We see the new component $\psi$ captures a level of space-time inseperability to the counts. At every point $(i, t)$ each component is modeled as coming from a mixture of two normal distributions, with the mixture component at the regional level,
\begin{equation}
\psi_{i,t} \sim z_i \cdot \operatorname{Normal}(0, \tau_1^2) + (1 - z_i) \cdot \operatorname{Normal}(0, \tau_2^2)
\end{equation} 

where
\begin{equation}
  z_i \sim \operatorname{Bernoulli}(q_i)
\end{equation}

and the $q_i$ are given Uniform priors on (0, 1). Here we are marginilizing out the $z_i$ and instead performing inference on the $q_i$ which confers the advantages set out under the computational considerations section. The variance parameters are given half-normal priors, one a vague prior (representing the abnormal regions) and the other an informative prior which restricts the inseperable variance of the `normal' regions to be very limited. Identification issues and the label switching problem are avoided by defining the larger of the variances addivitely in terms of the smaller:
\begin{align}
  \tau_1 &\sim \operatorname{Normal}(0, 0.01) \cdot I(0, \infty) \\
  k &\sim \operatorname{Normal}(0, 100) \cdot I(0, \infty)
\end{align}
\begin{equation}
 \tau_2 = \tau_2 + k
\end{equation}

Here $I$ is the indicator function.

\section{Classification and accuracy}

This model was fit using HMC in \emph{Stan} 

\section{Over-smoothing}

\begin{itemize}
\item Expected disease trends will cluster
\item Model relies on finding unseperable variance but local high variance could be explained by CAR component
\item Need a model which smooths regions but is able to still identify clustered unusual areas
\end{itemize}

\section{Simulated Data}

\begin{itemize}
\item Preferential attachment scheme
\item Describe full data generation parameters and process
\item Include plots of temporal and spatial components
\end{itemize}

\chapter{Computational considerations}

\section{Basic Sampling methods}

\begin{itemize}

\item Mainly using stan for the model

\item Also used BUGS for mixing and speed comparison

\item Discuss pymc3 too and compare with stan's parameter tuning/ initilization with ADVI

\end{itemize}

\subsubsection{Gibbs sampling}

\subsubsection{Metropolis}

<<<<<<< HEAD
=======

\subsubsection{Auto differentiation}

\subsubsection{Symplectic Integration}
>>>>>>> origin/master

\begin{itemize}
\item Leap-frog integration
\item Metropolis correction
\end{itemize}

<<<<<<< HEAD
\subsubsection{Auto differentiation}

\subsubsection{Symplectic Integration}

\begin{itemize}
\item Leap-frog integration
\item Metropolis correction
\end{itemize}

\subsubsection{Integration time}

\begin{itemize}
\item Naive implementations will not preserve detailed balance
\item Short-time will give large autocorrelations
\item Long-time will face diminishing returns
\item Integration time obviously linear in time, so compare to linear
\item Exhuastive monte-carlo
\item \emph{Identifying the Optimal Integration Time in Hamiltonian Monte-Carlo} (Betancourt 2016)
\end{itemize}

=======
\subsubsection{Integration time}

\begin{itemize}
\item Naive implementations will not preserve detailed balance
\item Short-time will give large autocorrelations
\item Long-time will face diminishing returns
\item Integration time obviously linear in time, so compare to linear
\item Exhuastive monte-carlo
\item \emph{Identifying the Optimal Integration Time in Hamiltonian Monte-Carlo} (Betancourt 2016)
\end{itemize}

>>>>>>> origin/master
\section{Autodiff/black-box variational inference?}

\section{Reparameterization of the models}

Removing conditional dependencies e.g.

\begin{align*}
\lambda \sim N(v, \sigma^2_\lambda) = N(0, 1) \cdot \sigma_\lambda^2 + v
\end{align*}

\section{Marginilizing over the discrte parameters}

As \emph{Stan} uses Hamiltonian Monte-Carlo, which is a gradient based sampler, we can't directly specify a discrete prior as used in mixture models. Instead we can ``marginalize out'' the mixture component to obtain a purely continuous distribution. This also has significan computational benefits (expand on this). \\

Take for example a mixture of two Normal distributions
\begin{equation}
  k \cdot \operatorname{Normal(\mu_1, \sigma_1^2)} + (1 - k) \cdot \operatorname{Normal(\mu_2, \sigma_2^2)}
\end{equation} 
\begin{equation}
  k \sim \operatorname{Bernoulli(\lambda)}.
\end{equation}

In Monte-Carlo we simply need to be able to calculate the posterior at a specific point. Most software, including \emph{Stan}, rather than multiply the posterior by a chain of conditional probability density functions works on the $\log$ scale where to calculate the log posterior we simply need to increment the log posterioir by the log conditional of the hierachical components. So for our mixture of Normals we need to implement the following
\begin{align}
  \text{log\_posterior} &= \text{log\_posterior} + \log(\lambda \cdot \operatorname{Normal\_pdf}(x | \mu_1, \sigma_1^2)  \nonumber \\
                   & \ \ + (1 - \lambda) \cdot \operatorname{Normal\_pdf}(x | \mu_2, \sigma_2^2))
\end{align}

which we can see is continuous. The way we can specifically implement this in \emph{Stan} is by using the following manipulation

\begin{align}
\log(p_X(x | \lambda, \mu_i, \sigma^2_i)) &= \log(\lambda \cdot \operatorname{Normal\_pdf}(x | \mu_1, \sigma_1^2)  \nonumber \\
& \ \ \ \ \ \ \ + (1 - \lambda) \cdot \text{Normal\_pdf}(x | \mu_2, \sigma_2^2)) \\
&= \log(\exp(\log(\lambda \cdot \operatorname{Normal\_pdf}(x | \mu_1, \sigma_1^2)))  \nonumber \\
& \ \ \ \ \ \ \ + \exp(\log((1 - \lambda) \cdot \operatorname{Normal\_pdf}(x | \mu_2, \sigma_2^2)))) \\
&= \operatorname{log\_sum\_exp}(\log(\lambda) + \log\operatorname{Normal\_pdf}(x | \mu_1, \sigma_1^2),  \nonumber \\
& \ \ \ \ \ \ \ \log(1 - \lambda) + \log\operatorname{Normal\_pdf}(x | \mu_2, \sigma_2^2))
\end{align} 

For more on this see page 185 of \cite{stan}.

\begin{itemize}

\item Cite Stan manual

\item Rao-Blackwellization? 

\end{itemize}

\chapter{Comparisions and conclusions}

Timing things

\printbibliography

\end{document}
